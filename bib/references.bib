@article{BargshadyGhazal2020Edla,
issn = {0957-4174},
abstract = {•Automated detection of pain from facial expressions is a challenge in medical care.•A new deep neural network algorithm designed to detection pain intensity effectively.•A new feature extraction algorithm developed to speed up the algorithm.•The new enhanced deep learning algorithm can detect pain in multi level effectively.
Automated detection of pain intensity from facial expressions, especially from face images that show a patient's health, remains a significant challenge in the medical diagnostics and health informatics area. Expert systems that prudently analyse facial expression images, utilising an automated machine learning algorithm, can be a promising approach for pain intensity analysis in health domain. Deep neural networks and emerging machine learning techniques have made significant progress in both the feature identification, mapping and the modelling of pain intensity from facial images, with great potential to aid health practitioners in the diagnosis of certain medical conditions. Consequently, there has been significant research within the pain recognition and management area that aim to adopt facial expression datasets into deep learning algorithms to detect the pain intensity in binary classes, and also to identify pain and non-pain faces. However, the volume of research in identifying pain intensity levels in multi-classes remains rather limited. This paper reports on a new enhanced deep neural network framework designed for the effective detection of pain intensity, in four-level thresholds using a facial expression image. To explore the robustness of the proposed algorithms, the UNBC-McMaster Shoulder Pain Archive Database, comprised of human facial images, was first balanced, then used for the training and testing of the classification model, coupled with the fine-tuned VGG-Face pre-trainer as a feature extraction tool. To reduce the dimensionality of the classification model input data and extract most relevant features, Principal Component Analysis was applied, improving its computational efficiency. The pre-screened features, used as model inputs, are then transferred to produce a new enhanced joint hybrid CNN-BiLSTM (EJH-CNN-BiLSTM) deep learning algorithm comprised of convolutional neural networks, that were then linked to the joint bidirectional LSTM, for multi-classification of pain. The resulting EJH-CNN-BiLSTM classification model, tested to estimate four different levels of pain, revealed a good degree of accuracy in terms of different performance evaluation techniques. The results indicated that the enhanced EJH-CNN-BiLSTM classification algorithm was explored as a potential tool for the detection of pain intensity in multi-classes from facial expression images, and therefore, can be adopted as an artificial intelligence tool in the medical diagnostics for automatic pain detection and subsequent pain management of patients.},
journal = {Expert systems with applications},
series ={Expert systems with applications},
pages = {113305},
volume = {149},
publisher = {Elsevier Ltd},
year = {2020},
title = {Enhanced deep learning algorithm development to detect pain intensity from facial expression images},
copyright = {2020 Elsevier Ltd},
language = {eng},
address = {New York},
author = {Bargshady, Ghazal and Zhou, Xujuan and Deo, Ravinesh C. and Soar, Jeffrey and Whittaker, Frank and Wang, Hua},
keywords = {type:Deep Learning Techniques,Algorithms , Artificial intelligence , Artificial neural networks , Automation , Classification , Deep learning , Deep neural networks , Expert systems , Expert systems in healthcare , Face recognition , Facial expression , Feature extraction , Machine learning , Mapping , Medical imaging , Medical research , Model testing , Neural networks , Pain , Pain detection , Performance evaluation , Principal components analysis},
}


@INPROCEEDINGS{9871770,
  author={Hosseini, Elahe and Fang, Ruijie and Zhang, Ruoyu and Chuah, Chen-Nee and Orooji, Mahdi and Rafatirad, Soheil and Rafatirad, Setareh and Homayoun, Houman},
  booktitle={2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Convolution Neural Network for Pain Intensity Assessment from Facial Expression}, 
  year={2022},
  volume={},
  number={},
  pages={2697-2702},
  series={IEEE},
  abstract={Pain is an unpleasant feeling that can reflect a patient's health situation. Since measuring pain is subjective, time-consuming, and needs continuous monitoring, automated pain intensity detection from facial expression holds great potential for smart healthcare applications. Convolutional Neural Networks (CNNs) are recently being used to identify features, map and model pain intensity from facial images, delivering great promise in helping practitioners detect disease. Limited research has been conducted to determine pain intensity levels across multiple classes. CNNs with simple learning schemes are limited in their ability to extract feature information from images. In order to develop a highly accurate pain intensity estimation system, this study proposes a Deep CNN (DCNN) model using the transfer learning technique, where a pre-trained DCNN model is adopted by replacing its dense upper layers, and the model is tuned using painful facial. We conducted experiments on the UNBC-McMaster shoulder pain archive database to estimate pain intensity in terms of seven-level thresholds using a given facial expression image. The experiments show our method achieves a promising improvement in terms of accuracy and performance to estimate pain intensity and outperform the-state-of-the-arts models.},
  keywords={type:Deep Learning Techniques,Training,Support vector machines,Pain,Databases,Transfer learning,Estimation,Shoulder},
  doi={10.1109/EMBC48229.2022.9871770},
  ISSN={2694-0604},
  month={July},}

@inproceedings{PataniaSabrina2022Dgnn,
abstract = {Automatic pain assessment can be defined as the set of computer aided technologies allowing to recognise pain status. Reliable and valid methods for pain assessment are of primary importance for the objective and continuous monitoring of pain in people who are unable to communicate verbally. In the present work, we propose a novel approach for the recognition of pain from the analysis of facial expression. More specifically, we evaluate the effectiveness of Graph Neural Network (GNN) architectures exploiting the inherent graph structure of a set of fiducial points automatically tracked on subject faces. Experiments carried over on the publicly available dataset BioVid, show how the proposed method reaches higher levels of accuracy when compared with baseline models on acted pain, while outmatching state of the art approaches on spontaneous pain.},
pages = {585--591},
publisher = {Assoc Computing Machinery},
booktitle = {37TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING},
isbn = {9781450387132},
year = {2022},
title = {Deep graph neural network for video-based facial pain expression assessment},
copyright = {Copyright 2023 Elsevier B.V., All rights reserved.},
language = {eng},
address = {NEW YORK},
series = {Assoc Computing Machinery},
author = {Patania, Sabrina and Boccignone, Giuseppe and Buršić, Sathya and D'Amelio, Alessandro and Lanzarotti, Raffaella},
keywords = {type:Deep Learning Techniques,automatic pain assessment , complexity-related measures , Computer Science , Computer Science Interdisciplinary Applications , Computer Science Theory & Methods , graph neural network , Science & Technology , spectral attributes , Technology},
organization = {ACM},
}


@article{OthmanEhsan2023Cnfc,
issn = {1047-3203},
abstract = {So far, the current methods in the clinical application do not facilitate continuous monitoring for pain and are unreliable, especially for vulnerable patients. In contrast, several automated methods have been proposed for this task by using facial features that were extracted independently from every frame of a given sequence. However, the obtained results were poor due to the failure to represent movement dynamics. To solve this problem, this work introduces three distinct methods regarding classification to monitor continuous pain intensity: (1) A Random Forest classifier (RFc) baseline method, (2) Long-Short Term Memory (LSTM) method, and (3) LSTM using sample weighting method (LSTM-SW). In this study, we conducted experiments with 11 datasets regarding classification, then compared results to regression results in Othman et al. (2021). Experimental results showed that the LSTM & LSTM-SW methods for continuous automatic pain intensity recognition performed better than guessing and RFc except with small datasets such as the reduced tonic datasets.
•Three automatic methods are introduced for continuous pain intensity recognition based on facial expression analysis.•Performance of classification and regression methods are compared.•Regression outperforms classification in imbalanced datasets.•RF is better than LSTM with a small dataset.},
journal = {Journal of visual communication and image representation},
series = {visual communication and image representation},
pages = {103743},
volume = {91},
publisher = {Elsevier Inc},
year = {2023},
title = {Classification networks for continuous automatic pain intensity monitoring in video using facial expression on the X-ITE Pain Database},
copyright = {2023 The Authors},
language = {eng},
address = {SAN DIEGO},
author = {Othman, Ehsan and Werner, Philipp and Saxen, Frerk and Al-Hamadi, Ayoub and Gruss, Sascha and Walter, Steffen},
keywords = {type:Deep Learning Techniques,Computer Science , Computer Science Information Systems , Computer Science Software Engineering , Continuous pain intensity recognition , Facial expression , Long-Short Term Memory , Random Forest classifier , Sample weighting , Science & Technology , Technology},
}

@INPROCEEDINGS{9631056,
  author={Xu, Xiaojing and de Sa, Virginia R.},
  booktitle={2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Personalized Pain Detection in Facial Video with Uncertainty Estimation}, 
  year={2021},
  volume={},
  number={},
  pages={4163-4168},
  abstract={Pain is a personal, subjective experience, and the current gold standard to evaluate pain is the Visual Analog Scale (VAS), which is self-reported at the video level. One problem with the current automated pain detection systems is that the learned model doesn’t generalize well to unseen subjects. In this work, we propose to improve pain detection in facial videos using individual models and uncertainty estimation. For a new test video, we jointly consider which individual models generalize well generally, and which individual models are more similar/accurate to this test video, in order to choose the optimal combination of individual models and get the best performance on new test videos. We show on the UNBC-McMaster Shoulder Pain Dataset that our method significantly improves the previous state-of-the-art performance.},
  keywords={type:Model Personalization,Visualization,Uncertainty,Pain,Biological system modeling,Estimation},
  series={IEEE},
  doi={10.1109/EMBC46164.2021.9631056},
  ISSN={2694-0604},
  month={Nov},}

@INPROCEEDINGS{10340872,
  author={Gkikas, Stefanos and Tsiknakis, Manolis},
  booktitle={2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={A Full Transformer-based Framework for Automatic Pain Estimation using Videos}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  series={IEEE},
  abstract={The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalization capability across all the primary pain estimation tasks.},
  keywords={type:Model Personalization,Pain,Computational modeling,Estimation,Transformers,Real-time systems,Task analysis,Videos},
  doi={10.1109/EMBC40787.2023.10340872},
  ISSN={2694-0604},
  month={July},}

@article{GKIKAS2023107365,
title = {Automatic assessment of pain based on deep learning methods: A systematic review},
journal = {Computer Methods and Programs in Biomedicine},
series = {Computer Methods and Programs in Biomedicine},
volume = {231},
pages = {107365},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107365},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723000329},
author = {Stefanos Gkikas and Manolis Tsiknakis},
keywords = {type:Comprehensive Reviews,Pain recognition, Affective computing, Machine learning, Facial expression, Biosignals},
abstract = {Background and Objective: The automatic assessment of pain is vital in designing optimal pain management interventions focused on reducing suffering and preventing the functional decline of patients. In recent years, there has been a surge in the adoption of deep learning algorithms by researchers attempting to encode the multidimensional nature of pain into meaningful features. This systematic review aims to discuss the models, the methods, and the types of data employed in establishing the foundation of a deep learning-based automatic pain assessment system. Methods: The systematic review was conducted by identifying original studies searching digital libraries, namely Scopus, IEEE Xplore, and ACM Digital Library. Inclusion and exclusion criteria were applied to retrieve and select those of interest, published until December 2021. Results: A total of one hundred and ten publications were identified and categorized by the number of information channels used (unimodal versus multimodal approaches) and whether the temporal dimension was also used. Conclusions: This review demonstrates the importance of multimodal approaches for automatic pain estimation, especially in clinical settings, and also reveals that significant improvements are observed when the temporal exploitation of modalities is included. It provides suggestions regarding better-performing deep architectures and learning methods. Also, it provides suggestions for adopting robust evaluation protocols and interpretation methods to provide objective and comprehensible results. Furthermore, the review presents the limitations of the available pain databases for optimally supporting deep learning model development, validation, and application as decision-support tools in real-life scenarios.}
}

@article{GoldsteinPavel2020ECTA,
issn = {1933-7213},
abstract = {Depression and anxiety co-occur with chronic pain, and all three are thought to be caused by dysregulation of shared brain systems related to emotional processing associated with body sensations. Understanding the connection between emotional states, pain, and bodily sensations may help understand chronic pain conditions. We developed a mobile platform for measuring pain, emotions, and associated bodily feelings in chronic pain patients in their daily life conditions. Sixty-five chronic back pain patients reported the intensity of their pain, 11 emotional states, and the corresponding body locations. These variables were used to predict pain 2 weeks later. Applying machine learning, we developed two predictive models of future pain, emphasizing interpretability. One model excluded pain-related features as predictors of future pain, and the other included pain-related predictors. The best predictors of future pain were interactive effects of (a) body maps of fatigue with negative affect and (b) positive affect with past pain. Our findings emphasize the contribution of emotions, especially emotional experience felt in the body, to understanding chronic pain above and beyond the mere tracking of pain levels. The results may contribute to the generation of a novel artificial intelligence framework to help in the development of better diagnostic and therapeutic approaches to chronic pain.},
journal = {Neurotherapeutics},
pages = {774--783},
volume = {17},
publisher = {Springer International Publishing},
number = {3},
year = {2020},
title = {Emerging Clinical Technology: Application of Machine Learning to Chronic Pain Assessments Based on Emotional Body Maps},
copyright = {The American Society for Experimental NeuroTherapeutics, Inc. 2020},
language = {eng},
address = {Cham},
series ={Neurotherapeutics},
author = {Goldstein, Pavel and Ashar, Yoni and Tesarz, Jonas and Kazgan, Mehmet and Cetin, Burak and Wager, Tor D.},
keywords = {type:Multi-Modal Approaches,Adult , Aged , Artificial intelligence , Back Pain - diagnosis , Back Pain - psychology , Biomedical and Life Sciences , Biomedical Technology - methods , Biomedical Technology - trends , Biomedicine , bodily sensation map , Chronic pain , Chronic Pain - diagnosis , Chronic Pain - psychology , Clinical Neurology , Emotions , Emotions - physiology , Female , Humans , interoception , Learning algorithms , Life Sciences & Biomedicine , low back pain , Machine learning , Machine Learning - trends , Male , Middle Aged , Neurobiology , Neurology , Neurosciences , Neurosciences & Neurology , Neurosurgery , Pain , pain assessment , Pain Measurement - methods , Pain Measurement - psychology , Pain Measurement - trends , Pharmacology & Pharmacy , Prediction models , Review , Science & Technology , Young Adult},
}

@INPROCEEDINGS{10061135,
  author={Rajyalakshmi, Chepuri and LakshmiNadh, K. and Reddy, M Sathyam},
  booktitle={2023 5th International Conference on Smart Systems and Inventive Technology (ICSSIT)}, 
  title={Deep Learning Model for Emotion Prediction from Speech, Facial Expression and Videos}, 
  year={2023},
  volume={},
  number={},
  pages={1084-1088},
  abstract={The rapid development of computer vision and machine learning in recent years has led to fruitful accomplishments in a variety of tasks, including the classification of objects, the identification of actions, and the recognition of faces, among other things. Nevertheless, identifying human emotions remains one of the most difficult tasks to do. To find a solution to this issue, a significant amount of work has been put in. In order to achieve higher accuracy in this reactivity towards a variety of speeches and vocal -based methods, computer intelligence, natural language modelling systems, and other similar technologies have been used. The examination of the emotions has the potential to be useful in a number of different settings. Cooperation with human computers is one example of such a field. Computers can help customers recognize emotions, make wiser decisions, and create more lifelike human-robot interactions. In recent times, there has been a lot of focus placed on the ability to forecast dynamic facial emotion expressions in videos. Therefore, this work proposes a deep convolutional neural networks (CNNs) model for emotion prediction from speech samples, facial expression images, and videos with enhanced prediction accuracy and reduced loss. In addition, the speech CNN model also utilizes mel-frequency Cepstrum coefficients (MFCC) as feature extraction from given speech samples. The proposed MFCC-CNN model resulted in superior performance than traditional models.},
  keywords={type:Multi-Modal Approaches,Emotion recognition,Visualization,Computational modeling,Predictive models,Ions,Feature extraction,Convolutional neural networks,Speech emotion,facial emotion,convolutional neural network,Mel Frequency Cepstral Coefficient,speech emotion recognition},
  doi={10.1109/ICSSIT55814.2023.10061135},
  ISSN={2832-3017},
  month={Jan},}

@article{Nerella2022EndtoEndML,
  title={End-to-End Machine Learning Framework for Facial AU Detection in Intensive Care Units},
  abstract={Pain is a common occurrence among patients admitted to Intensive Care Units. Pain assessment in ICU patients still remains a challenge for clinicians and ICU staff, specifically in cases of non-verbal sedated, mechanically ventilated, and intubated patients. Current manual observation-based pain assessment tools are limited by the frequency of pain observations administered and are subjective to the observer. Facial behavior is a major component in observation-based tools. Furthermore, previous literature shows the feasibility of painful facial expression detection using facial action units (AUs). However, these approaches are limited to controlled or semi-controlled environments and have never been validated in clinical settings. In this study, we present our Pain-ICU dataset, the largest dataset available targeting facial behavior analysis in the dynamic ICU environment. Our dataset comprises 76,388 patient facial image frames annotated with AUs obtained from 49 adult patients admitted to ICUs at the University of Florida Health Shands hospital. In this work, we evaluated two vision transformer models, namely ViT and SWIN, for AU detection on our Pain-ICU dataset and also external datasets. We developed a completely end-to-end AU detection pipeline with the objective of performing real-time AU detection in the ICU. The SWIN transformer Base variant achieved 0.88 F1-score and 0.85 accuracy on the held-out test partition of the Pain-ICU dataset.},
  author={Subhash Nerella and Kia Khezeli and Andrea Davidson and Patrick James Tighe and Azra Bihorac and Parisa Rashidi},
  journal={ArXiv},
  series={ArXiv},
  year={2022},
  volume={abs/2211.06570},
  keywords={type:Multi-Modal Approaches,AU detection , Pain , ICU , Computer vision , Transformers},
  url={https://api.semanticscholar.org/CorpusID:253510734}
}